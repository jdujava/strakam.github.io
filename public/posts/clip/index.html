<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>CLIP: Contrastive Language-Image Pretraining | Magpie</title>
<meta name="keywords" content="">
<meta name="description" content="A short introduction to CLIP, the model behind current State-of-the-Art Computer Vision models.">
<meta name="author" content="Matej Straka">
<link rel="canonical" href="http://localhost:1313/posts/clip/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.12907bd721a648a1d1bebe5281debfbed49fb1633f2af0511b280dc980ae310a.css" integrity="sha256-EpB71yGmSKHRvr5Sgd6/vtSfsWM/KvBRGygNyYCuMQo=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/posts/clip/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript>
    
</head>

<body class="" id="top">

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="Magpie (Alt + H)">Magpie</a>
            <div class="logo-switches">
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/archives/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/about/" title="About">
                    <span>About</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      CLIP: Contrastive Language-Image Pretraining
    </h1>
    <div class="post-description">
      A short introduction to CLIP, the model behind current State-of-the-Art Computer Vision models.
    </div>
    <div class="post-meta"><span title='2024-04-19 00:00:00 +0000 UTC'>April 19, 2024</span>&nbsp;Â·&nbsp;Matej Straka

</div>
  </header> 
  <div class="post-content"><h2 id="motivation">Motivation<a hidden class="anchor" aria-hidden="true" href="#motivation">#</a></h2>
<p>Ever since Deep Learning started as a field, we have been observing an increased performance of models coming
from using bigger datasets and more compute. One such compute and data demanding architecture, Transformer [9],
is used in many State-of-the-Art methods. The architecture properties and recent scaling laws [3] indicate that
leveraging large amounts of data should lead to increased performance.</p>
<p>However, the problem with datasets such as ImageNet is that they grow slowly and have a weak annotation
(usually just one word). On the other hand, websites such as Wikipedia or Instagram, contain bilions of images that
contain some richer annotation in form of captions. CLIP leverages these large &lsquo;annotated&rsquo; datasets to train large
vision and language transformers in a fashion from which interesting properties emerge.</p>
<h3 id="shared-latent-space">Shared Latent Space<a hidden class="anchor" aria-hidden="true" href="#shared-latent-space">#</a></h3>
<p>The first interesting property is that CLIP doesn&rsquo;t just learn arbitrary representations of text and images,
but it embeds them into the same latent space. This basically means that when we get vector representation from CLIP
for text and for image, we can compute the similarity between using cosine similarity. This enables us to use CLIP
for text-image retrieval or text conditioning in image generation or image segmentation.</p>
<h3 id="zero-shot-transfer">Zero-Shot Transfer<a hidden class="anchor" aria-hidden="true" href="#zero-shot-transfer">#</a></h3>
<p>The second crucial property of CLIP is that it unlocks zero-shot transfer. In practice, instead of encoding image classes
as one-hot vectors, we can encode them simply as &lsquo;strings&rsquo;. Then, since CLIP text encoder can encode any text, it can
therefore encode any label we want. This allows us to use CLIP on our custom labels without any transfer learning.
See code down below.</p>
<h2 id="method">Method<a hidden class="anchor" aria-hidden="true" href="#method">#</a></h2>
<h2 id="main-results">Main results<a hidden class="anchor" aria-hidden="true" href="#main-results">#</a></h2>
<h2 id="roll-your-own-classifier">Roll Your Own Classifier!<a hidden class="anchor" aria-hidden="true" href="#roll-your-own-classifier">#</a></h2>
<h2 id="conclusion">Conclusion<a hidden class="anchor" aria-hidden="true" href="#conclusion">#</a></h2>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="http://localhost:1313/">Magpie</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
</body>

</html>
