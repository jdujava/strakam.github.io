<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>CLIP: Contrastive Language-Image Pretraining | Magpie</title>
<meta name="keywords" content="">
<meta name="description" content="A short introduction to CLIP, the model behind current State-of-the-Art Computer Vision models.">
<meta name="author" content="Matej Straka">
<link rel="canonical" href="http://localhost:1313/posts/clip/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.890b189f73ecfcce87f8e4a04a7efd42ffd035db21a8b95be2c76ccac2042bca.css" integrity="sha256-iQsYn3Ps/M6H&#43;OSgSn79Qv/QNdshqLlb4sdsysIEK8o=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/posts/clip/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript>
    
    
      <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
<script>
  MathJax = {
    tex: {
      displayMath: [['\\[', '\\]'], ['$$', '$$']],  
      inlineMath: [['$', '$']]                  
    }
  };
</script>
    
</head>

<body class="" id="top">

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="Magpie (Alt + H)">Magpie</a>
            <div class="logo-switches">
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/archives/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/about/" title="About">
                    <span>About</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      CLIP: Contrastive Language-Image Pretraining
    </h1>
    <div class="post-description">
      A short introduction to CLIP, the model behind current State-of-the-Art Computer Vision models.
    </div>
    <div class="post-meta"><span title='2024-04-19 00:00:00 +0000 UTC'>April 19, 2024</span>&nbsp;·&nbsp;Matej Straka

</div>
  </header> 
  <div class="post-content"><h3 id="disclaimer">Disclaimer<a hidden class="anchor" aria-hidden="true" href="#disclaimer">#</a></h3>
<p>This is my first ever post, please excuse any inaccuracies. Any suggestions welcome at <a href="mailto:strakammm@gmail.com">strakammm@gmail.com</a>!</p>
<h2 id="motivation">Motivation<a hidden class="anchor" aria-hidden="true" href="#motivation">#</a></h2>
<p>Ever since Deep Learning started as a field, we have been observing an <strong>increased performance</strong> of models
coming from using <strong>bigger datasets</strong> and <strong>more compute</strong>. One such compute and data demanding architecture, <a href="https://arxiv.org/abs/1706.03762">Transformer</a>,
is used in many State-of-the-Art methods. The architecture properties and recent <a href="https://arxiv.org/abs/2001.08361">scaling laws</a> indicate that
leveraging large amounts of data should lead to increased performance.</p>
<p>However, the problem with datasets such as <a href="https://arxiv.org/abs/1409.0575">ImageNet</a> is that <strong>they grow slowly</strong> and <strong>have a weak annotation</strong>
(usually just one word). On the other hand, websites such as Wikipedia or Instagram, <strong>contain bilions of images</strong> that
contain <strong>richer annotation</strong> in form of <strong>captions</strong>. <a href="https://arxiv.org/abs/2103.00020">CLIP</a>, a multi-modal model, leverages these large
annotated datasets to train large vision and language transformers in a fashion from which interesting properties emerge.</p>
<h3 id="shared-latent-space">Shared Latent Space<a hidden class="anchor" aria-hidden="true" href="#shared-latent-space">#</a></h3>
<p>The first interesting property is that CLIP doesn&rsquo;t just learn <strong>arbitrary representations</strong> of text and images,
but it <strong>embeds them into the same latent space</strong>. This basically means that when we get vector representation from CLIP
for text and for image, we can compute the similarity between using cosine similarity. This enables us to use CLIP
for <strong>text-image retrieval</strong> or <strong>text conditioning</strong> in <strong>image generation</strong> or <strong>image segmentation</strong>.</p>
<p><img loading="lazy" src="images/latent.png" alt="Shared Latent Space"  />
</p>
<h3 id="zero-shot-transfer">Zero-Shot Transfer<a hidden class="anchor" aria-hidden="true" href="#zero-shot-transfer">#</a></h3>
<p>The second crucial property of CLIP is that it <strong>unlocks zero-shot transfer</strong>. In practice, instead of encoding image classes
as one-hot vectors, we can encode them simply as <em>strings</em>. Then, since CLIP <strong>text encoder can encode any text</strong>, it can
therefore <strong>encode any label</strong> we want. This allows us to use CLIP on our custom labels <strong>without any transfer learning</strong>.
See code down below.</p>
<h2 id="method">Method<a hidden class="anchor" aria-hidden="true" href="#method">#</a></h2>
<p><a name="diagram_anchor"></a></p>
<h3 id="pre-training">Pre-Training<a hidden class="anchor" aria-hidden="true" href="#pre-training">#</a></h3>
<p>The training procedure for the CLIP method is very similar to traditional supervised training.
The dataset consists of 400 milion image-text pairs scraped from the internet.</p>
<p>The architecture leverages one encoder for text and one for images. After obtaining a batch
of N image-text pairs, we obtain embeddings from respective encoders.
These representations are projected by one more linear layer into the <strong>same latent space</strong> where they are
$L_2$ normalized.</p>
<p>From these normalized representations an
$N × N$ <strong>similarity matrix</strong> is created by computing <strong>pair-wise dot-product</strong>
between every image and text representation.
Then the <code>softmax</code> operation is applied to <strong>every row and column</strong>,
creating probability distributions - for each image we
have distribution over all texts and symmetrically for each
text we have distribution over all images. Logits of ground
truth labels are conveniently on the diagonal of the similarity matrix,
from which we compute <strong>cross-entropy loss</strong> and <strong>train both encoders jointly</strong>.</p>
<p><img loading="lazy" src="images/main-diagrams.png" alt="CLIP diagram"  />
</p>
<h3 id="downstream-adaptation">Downstream adaptation<a hidden class="anchor" aria-hidden="true" href="#downstream-adaptation">#</a></h3>
<p>On the right side of the image above we observe how such model
can be applied to downstream tasks.
We create <strong>custom labels as strings</strong>, encode them <strong>once</strong> and cache obtained representations.
Then, when a new image that has to be classified comes in, we obtain its representation and compare
it to cached text embeddings. Since <strong>text encoder can encode any string of characters</strong> to obtain
its representation, <strong>the zero-shot property emerges</strong>.</p>
<h2 id="main-results">Main Results<a hidden class="anchor" aria-hidden="true" href="#main-results">#</a></h2>
<h3 id="zero-shot-transfer-1">Zero-Shot Transfer<a hidden class="anchor" aria-hidden="true" href="#zero-shot-transfer-1">#</a></h3>
<p>Perhaps the most important result of the paper is about its
<strong>zero-shot performance</strong>. Authors stress the importance of
zero-shot evaluation as a way to assess true generalization
strength of Computer Vision models on unseen datasets and
motivate it as measuring <strong>task-learning capabilities</strong>. This is
important because
<em>true general models should not need to be adapted to every new task they are presented with</em>
From the following figure we can see that CLIP is a <strong>very strong zero-shot</strong>
model, outperforming ResNet-50 on majority of datasets:</p>
<img src="images/zs.png" style="width: 400px; margin: auto">
<h3 id="natural-distribution-shift">Natural Distribution Shift<a hidden class="anchor" aria-hidden="true" href="#natural-distribution-shift">#</a></h3>
<p>The study of generalization suggests another concept called
<em><strong>Natural Distribution Shift</strong></em>, which measures models’ ability
to classify images from the <strong>same class</strong>, but sampled from
a <strong>different distribution</strong> (dataset). Consider an example of a
banana. In one dataset, we might be presented with a photo
of a banana, but in another, there might be black and white
sketches of it. Humans understand the concept of a sketch
and therefore understand that it does not need to be yellow.
But do Computer Vision models understand it too?
<img loading="lazy" src="images/rd.png" alt="Natural Distribution Shift"  />
</p>
<p>Authors show
that most Computer Vision models trained on ImageNet are <strong>weak</strong>
in this regard. CLIP, on the other hand, is <strong>able to maintain
its performance</strong> when presented with samples for various distributions,
which hints that it is indeed a <strong>very general model</strong>.</p>
<h2 id="roll-your-own-classifier">Roll Your Own Classifier!<a hidden class="anchor" aria-hidden="true" href="#roll-your-own-classifier">#</a></h2>
<p>The nature of CLIP allows us to create our own classifier very quickly.
Following code is a simple example of how to do it using PyTorch&#x1f525; and HuggingFace🤗:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Import the CLIP model</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">CLIPProcessor</span><span class="p">,</span> <span class="n">CLIPModel</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Processor tokenizes the labels and prepares images for model</span>
</span></span><span class="line"><span class="cl"><span class="n">processor</span> <span class="o">=</span> <span class="n">CLIPProcessor</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&#34;openai/clip-vit-base-patch32&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Model actually performs the forward pass and gives predictions</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="n">CLIPModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&#34;openai/clip-vit-base-patch32&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Load our image, which is a panda eating a bamboo stick</span>
</span></span><span class="line"><span class="cl"><span class="n">image</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s1">&#39;panda.jpg&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Define our own labels as we want</span>
</span></span><span class="line"><span class="cl"><span class="n">my_own_labels</span> <span class="o">=</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;a photo of a panda&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;a photo of a grizzly bear&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;a photo of a bamboo&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Prepare inputs for the forward pass</span>
</span></span><span class="line"><span class="cl"><span class="n">inputs</span> <span class="o">=</span> <span class="n">processor</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">text</span><span class="o">=</span><span class="n">my_own_labels</span><span class="p">,</span> <span class="n">images</span><span class="o">=</span><span class="n">image</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&#34;pt&#34;</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span> <span class="c1"># Apply forward pass</span>
</span></span><span class="line"><span class="cl"><span class="n">logits_per_image</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">logits_per_image</span>  <span class="c1"># Obtain the logits</span>
</span></span><span class="line"><span class="cl"><span class="n">probs</span> <span class="o">=</span> <span class="n">logits_per_image</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># Convert to probabilities</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">probs</span><span class="p">)</span>
</span></span></code></pre></div><p>which outputs:</p>
<p><code>tensor([[7.8885e-01, 1.9084e-04, 2.1096e-01]], grad_fn=&lt;SoftmaxBackward0&gt;)</code></p>
<p>This essentially says that there is a</p>
<ul>
<li>~78.9% that the image is a photo of a panda</li>
<li>~0.0001% that the image is a photo of a grizzly bear</li>
<li>~21% that the image is a photo of a bamboo</li>
</ul>
<p>And that is it! This is how you can create your own classifier for your next project!</p>
<h2 id="conclusion">Conclusion<a hidden class="anchor" aria-hidden="true" href="#conclusion">#</a></h2>
<p>In this short post we have motivated and desribed the <em><strong>CLIP method</strong></em>.
The CLIP method is a powerful tool for Computer Vision.
The ideas for pre-training show us how we can leverage vast amounts
of data that is freely available on the internet.
The zero-shot performance gives us a much more representative measure
of generality of Computer Vision models and Deep Learning practitioners
can leverage zero-shot for quickly evaluating new ideas.</p>
<h2 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h2>
<ul>
<li>
<p><a href="https://arxiv.org/abs/1706.03762">Attention is all you need</a></p>
</li>
<li>
<p><a href="https://arxiv.org/abs/2103.00020">Learning Transferable Visual Models From Natural Language Supervision</a></p>
</li>
<li>
<p><a href="https://arxiv.org/pdf/2001.08361">Scaling Laws for Neural Language Models</a></p>
</li>
<li>
<p><a href="https://arxiv.org/abs/1409.0575">ImageNet</a></p>
</li>
</ul>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="http://localhost:1313/">Magpie</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
</body>

</html>

  