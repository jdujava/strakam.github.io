[{"content":"Motivation Ever since Deep Learning started as a field, we have been observing an increased performance of models coming from using bigger datasets and more compute. One such compute and data demanding architecture, Transformer [9], is used in many State-of-the-Art methods. The architecture properties and recent scaling laws [3] indicate that leveraging large amounts of data should lead to increased performance.\nHowever, the problem with datasets such as ImageNet is that they grow slowly and have a weak annotation (usually just one word). On the other hand, websites such as Wikipedia or Instagram, contain bilions of images that contain some richer annotation in form of captions. CLIP leverages these large \u0026lsquo;annotated\u0026rsquo; datasets to train large vision and language transformers in a fashion from which interesting properties emerge.\nShared Latent Space The first interesting property is that CLIP doesn\u0026rsquo;t just learn arbitrary representations of text and images, but it embeds them into the same latent space. This basically means that when we get vector representation from CLIP for text and for image, we can compute the similarity between using cosine similarity. This enables us to use CLIP for text-image retrieval or text conditioning in image generation or image segmentation.\nZero-Shot Transfer The second crucial property of CLIP is that it unlocks zero-shot transfer. In practice, instead of encoding image classes as one-hot vectors, we can encode them simply as \u0026lsquo;strings\u0026rsquo;. Then, since CLIP text encoder can encode any text, it can therefore encode any label we want. This allows us to use CLIP on our custom labels without any transfer learning. See code down below.\nMethod Main results Roll Your Own Classifier! Conclusion ","permalink":"http://localhost:1313/posts/clip/","summary":"Motivation Ever since Deep Learning started as a field, we have been observing an increased performance of models coming from using bigger datasets and more compute. One such compute and data demanding architecture, Transformer [9], is used in many State-of-the-Art methods. The architecture properties and recent scaling laws [3] indicate that leveraging large amounts of data should lead to increased performance.\nHowever, the problem with datasets such as ImageNet is that they grow slowly and have a weak annotation (usually just one word).","title":"CLIP: Contrastive Language-Image Pretraining"},{"content":"","permalink":"http://localhost:1313/posts/test/","summary":"","title":"Riadna testazz"},{"content":" Currently Standing on the shoulders of giants\nEducation 2019 - 2022 (Bachelors) Faculty of Mathematics and Physics, Charles University\nGeneral Computer Science, Combinatorics 2022 - Present (Masters) Faculty of Mathematics and Physics, Charles University\nDeep Learning, Reinforcement Learning 2023-2024 Technical University of Munich\nErasmus for an academic year Working Experience Aug 2022 - Aug 2023 DataSentics, an Eviden business\nData Scientist Focus on Computer Vision and Apache Spark pipelines Activities Teacher assistant: Deep Learning, Machine Learning at MFF CUNI\nksp.sk Successful participant: Seminar on algorithmic programming\nkasiopea.matfyz.cz Successful participant: Competitive programming, national round\nksi.fi.muni.cz Successful participant: Informatics seminar by MUNI\n","permalink":"http://localhost:1313/cv/","summary":"Currently Standing on the shoulders of giants\nEducation 2019 - 2022 (Bachelors) Faculty of Mathematics and Physics, Charles University\nGeneral Computer Science, Combinatorics 2022 - Present (Masters) Faculty of Mathematics and Physics, Charles University\nDeep Learning, Reinforcement Learning 2023-2024 Technical University of Munich\nErasmus for an academic year Working Experience Aug 2022 - Aug 2023 DataSentics, an Eviden business\nData Scientist Focus on Computer Vision and Apache Spark pipelines Activities Teacher assistant: Deep Learning, Machine Learning at MFF CUNI","title":""},{"content":"Main interests Deep Learning Reinforcement Learning Computer Vision Lots of ad hoc stuff connected to topics above Activities Teacher assistant: Deep Learning, Machine Learning at MFF CUNI\nSuccessful participant:\nSeminar on algorithmic programming ksp.sk\nCompetitive programming, national round kasiopea.matfyz.cz\nInformatics seminar by MUNI ksi.fi.muni.cz\nSkills ","permalink":"http://localhost:1313/about/","summary":"Main interests Deep Learning Reinforcement Learning Computer Vision Lots of ad hoc stuff connected to topics above Activities Teacher assistant: Deep Learning, Machine Learning at MFF CUNI\nSuccessful participant:\nSeminar on algorithmic programming ksp.sk\nCompetitive programming, national round kasiopea.matfyz.cz\nInformatics seminar by MUNI ksi.fi.muni.cz\nSkills ","title":"About"}]